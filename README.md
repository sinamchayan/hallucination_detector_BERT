# Hallucination Detector (BERT - TensorFlow)

Detect false information (hallucinations) in AI-generated summaries using Natural Language Inference with TensorFlow BERT.

This project leverages a fine-tuned BERT model to classify the consistency between a source document and a generated summary. By treating the problem as a Natural Language Inference (NLI) task, the system can determine if the summary is entailed by the source (faithful) or contradicts it (hallucination). It serves as a critical quality assurance layer for LLM pipelines, ensuring the reliability of automated text generation in professional and academic contexts.

## Features

- âœ… Single & batch processing
- ğŸ“Š Interactive Streamlit dashboard
- ğŸš€ FastAPI REST API
- ğŸ“ˆ MLflow experiment tracking
- ğŸ³ Docker deployment ready
- ğŸ§  TensorFlow/Keras BERT model

## Quick Start

### 1. Install Dependencies
```bash
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Train Model (30-60 minutes)
```bash
python run.py train
```

### 3. Start API
```bash
python run.py api
```

### 4. Start UI (new terminal)
```bash
python run.py ui
```

Open browser at `http://localhost:8501`

## Project Directory

```
â”œâ”€â”€ data/               # Data storage
â”œâ”€â”€ docker/             # Docker configuration
â”œâ”€â”€ logs/               # Application logs
â”œâ”€â”€ mlruns/             # MLflow experiments
â”œâ”€â”€ models/             # Saved models
â”œâ”€â”€ src/                # Source code
â”‚   â”œâ”€â”€ api.py          # FastAPI application
â”‚   â”œâ”€â”€ model.py        # BERT model implementation
â”‚   â””â”€â”€ ui.py           # Streamlit dashboard
â”œâ”€â”€ utils/              # Utility functions
â”‚   â”œâ”€â”€ config.py       # Configuration settings
â”‚   â””â”€â”€ logger.py       # Logging setup
â”œâ”€â”€ run.py              # Main entry point script
â”œâ”€â”€ start_app.sh        # Startup script
â””â”€â”€ requirements.txt    # Project dependencies
```

## Project Workflow

The project is designed to be modular. You can run components individually or use the helper script.

### Automated Startup
To start both the API and UI simultaneously:
```bash
chmod +x start_app.sh
./start_app.sh
```

### Manual Workflow
1. **Training Phase**: The model must be trained first to generate the necessary artifacts in `models/`.
   ```bash
   python run.py train
   ```
2. **Serving Phase**:
   - **API**: Handles inference requests.
   - **UI**: Provides a user interface for interaction.
